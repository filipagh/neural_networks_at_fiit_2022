{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 2\n",
    "Implement basic backward pass using only numpy:\n",
    " - for your last week's Single Layer Perceptron.\n",
    " - for all activation functions\n",
    " - loss functions\n",
    "\n",
    "Perform forward pass and backward pass, and use the gradient check function to verify your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Layer\n",
    "\n",
    "In your previous task you defined `forward(input)` pass for your Linear class. Now we continue in creation of your own framework a little further with defining the `backward(dNet)` function. In this little framework is activation and linear unit separated. This separation is benefit in backward propagation and optimization. (If you want to know why, take a look on implementation of forward and backward propagation in class Model.)\n",
    "\n",
    "Note, that you are implementing backward pass for the whole dataset of `m` samples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear class\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.b = np.zeros((out_features, 1)) # Watch-out for the shape\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_inputs = input\n",
    "        self.m = self.fw_inputs.shape[1]\n",
    "        net = np.matmul(self.W, input) + self.b #z\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        self.db = 1/self.m * np.sum(dz,axis=1, keepdims=1)\n",
    "        self.dW = 1/self.m * np.matmul(dz, self.fw_inputs.T)\n",
    "        print(\"aaaaaaaa\")\n",
    "        print(np.shape(self.W.T))\n",
    "        print(np.shape(dz))\n",
    "        matmul = np.matmul(self.W.T, dz)\n",
    "        print(np.shape(matmul))\n",
    "        return matmul\n",
    "        # <<<<<<<<<"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activations\n",
    "Implement backward pass for Sigmoid, Tanh and ReLU activation functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        f = self.fw_input\n",
    "        return da * f * (1 - f)\n",
    "        # <<<<<<<<<\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "def htan(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2 * input) - 1) / (np.exp(2 * input) + 1)\n",
    "\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        x = self.fw_input\n",
    "        return da * ( 1 - htan(x) * htan(x))\n",
    "        pass\n",
    "        # <<<<<<<<<\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        x =self.fw_input\n",
    "        return da * ((x > 0) * 1)\n",
    "        # <<<<<<<<<"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss functions\n",
    "For successful backward pass, the computation and derivation of Loss function is necessary.\n",
    "The most common Loss functions are **Mean Square Error** _(MSE, L2)_ **Mean Absolute Error** _(MAE, L1)_ and **Binary Cross Entropy** _(BCE, Log Loss)_ and their modifications according to what is better for the current dataset.\n",
    "\n",
    "Implement MSE and BCE Loss functions as Modules of our little framework.\n",
    "\n",
    "Remember the difference between Loss and Cost."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        return np.square(np.subtract(target,input)).mean()\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        return 2*(input-target)/input.shape[1]\n",
    "        # <<<<<<<<<\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        y = target\n",
    "        a= input\n",
    "        return -(y*np.log(a) + (1-y)* log(1-a))\n",
    "\n",
    "        y_pred = input\n",
    "        y_true = target\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        term_0 = (1-y_true) * np.log(1-y_pred + 1e-7)\n",
    "        term_1 = y_true * np.log(y_pred + 1e-7)\n",
    "        return -np.mean(term_0+term_1, axis=0)\n",
    "        # <<<<<<<<<\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # >>>>>>>>> add here\n",
    "        y = target\n",
    "        a= input\n",
    "        return -y / a + (1-y) / (1-a)\n",
    "\n",
    "        # return np.asscalar((-1/len(Z)) * (np.dot(Y, np.log(Z + (1.e-10))) + np.dot((1 - Y), np.log(1 - Z + (1.e-10)))))\n",
    "\n",
    "    pass\n",
    "        # <<<<<<<<<"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "As in previous task, use `Model` class to encapsulate all layers of your MLP and define backward pass.\n",
    "Iterate over its modules stored in parameter OrderedDict `modules` -> `self.modules` in the correct order.\n",
    "\n",
    "Use call `.add_module(...)` to add layers of your MLP (network). Define MLP that could classify data from Circles dataset `dataset_Circles(...)`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            print(f'Layer fw:{name}, a.shape = {input.shape} \\n')\n",
    "            input = module.forward(input)\n",
    "            print(f'Layer fw:{name} OUT, a.shape = {input.shape} \\n')\n",
    "\n",
    "            # print(f'z.shape = {input.shape} \\n{input}')\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        # >>>>>>>>> add here\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            print(f'Layer bw:{name}, a.shape = {z.shape} \\n')\n",
    "            input = module.backward(z)\n",
    "            print(f'Layer bw:{name} OUT, a.shape = {z.shape} \\n')\n",
    "\n",
    "            # print(f'z.shape = {input.shape} \\n{input}')\n",
    "        return input\n",
    "        pass\n",
    "        # <<<<<<<<<"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Processing Cell\n",
    "\n",
    " 1. Initialize dataset (`dataset_Circles`). [x]\n",
    " 2. Declare a simple model (at least 3 hidden layer). [x]\n",
    " 3. Perform forward pass through the network. [x]\n",
    " 4. Compute loss. []\n",
    " 5. Backward prop loss. []\n",
    " 6. Backward pass MLP. []\n",
    " 7. Check your computation of gradients via [`gradient_check`](https://datascience-enthusiast.com/DL/Improving_DeepNeural_Networks_Gradient_Checking.html) []\n",
    " 8. Start crying. []\n",
    " 9. Repeat until correct ;) []\n",
    " 10. ... (if error founds -> blame lecturer) []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "import pandas\n",
    "from dataset import dataset_Circles\n",
    "from utils import gradient_check\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer fw:Dense_1, a.shape = (2, 128) \n",
      "\n",
      "Layer fw:Dense_1 OUT, a.shape = (3, 128) \n",
      "\n",
      "Layer fw:Tanh_1, a.shape = (3, 128) \n",
      "\n",
      "Layer fw:Tanh_1 OUT, a.shape = (3, 128) \n",
      "\n",
      "Layer fw:Dense_2, a.shape = (3, 128) \n",
      "\n",
      "Layer fw:Dense_2 OUT, a.shape = (4, 128) \n",
      "\n",
      "Layer fw:Tanh_2, a.shape = (4, 128) \n",
      "\n",
      "Layer fw:Tanh_2 OUT, a.shape = (4, 128) \n",
      "\n",
      "Layer fw:Dense_3, a.shape = (4, 128) \n",
      "\n",
      "Layer fw:Dense_3 OUT, a.shape = (5, 128) \n",
      "\n",
      "Layer fw:Tanh_3, a.shape = (5, 128) \n",
      "\n",
      "Layer fw:Tanh_3 OUT, a.shape = (5, 128) \n",
      "\n",
      "Layer fw:Dense_4_out, a.shape = (5, 128) \n",
      "\n",
      "Layer fw:Dense_4_out OUT, a.shape = (1, 128) \n",
      "\n",
      "Layer fw:Sigmoid, a.shape = (1, 128) \n",
      "\n",
      "Layer fw:Sigmoid OUT, a.shape = (1, 128) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_features_X, dataset_labels_Y = dataset_Circles(m=128, radius=0.7, noise=0.0)\n",
    "\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlp.add_module(Tanh(), 'Tanh_1')\n",
    "mlp.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlp.add_module(Tanh(), 'Tanh_2')\n",
    "mlp.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlp.add_module(Tanh(), 'Tanh_3')\n",
    "mlp.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlp.add_module(Sigmoid(), 'Sigmoid')\n",
    "\n",
    "predicted_Y_hat = mlp.forward(dataset_features_X) # Be careful with the shape - Loss vs Cost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer bw:Sigmoid, a.shape = (1, 128) \n",
      "\n",
      "Layer bw:Sigmoid OUT, a.shape = (1, 128) \n",
      "\n",
      "Layer bw:Dense_4_out, a.shape = (1, 128) \n",
      "\n",
      "aaaaaaaa\n",
      "(5, 1)\n",
      "(1, 128)\n",
      "(5, 128)\n",
      "Layer bw:Dense_4_out OUT, a.shape = (1, 128) \n",
      "\n",
      "Layer bw:Tanh_3, a.shape = (1, 128) \n",
      "\n",
      "Layer bw:Tanh_3 OUT, a.shape = (1, 128) \n",
      "\n",
      "Layer bw:Dense_3, a.shape = (1, 128) \n",
      "\n",
      "aaaaaaaa\n",
      "(4, 5)\n",
      "(1, 128)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [88]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m MSELoss()\n\u001B[1;32m      5\u001B[0m loss_fn\u001B[38;5;241m.\u001B[39mforward( predicted_Y_hat,dataset_labels_Y)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mmlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredicted_Y_hat\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdataset_labels_Y\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [85]\u001B[0m, in \u001B[0;36mModel.backward\u001B[0;34m(self, z)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mitems()):\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLayer bw:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, a.shape = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mz\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLayer bw:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m OUT, a.shape = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mz\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# print(f'z.shape = {input.shape} \\n{input}')\u001B[39;00m\n",
      "Input \u001B[0;32mIn [82]\u001B[0m, in \u001B[0;36mLinear.backward\u001B[0;34m(self, dz)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mshape(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW\u001B[38;5;241m.\u001B[39mT))\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mshape(dz))\n\u001B[0;32m---> 27\u001B[0m matmul \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdz\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mshape(matmul))\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m matmul\n",
      "\u001B[0;31mValueError\u001B[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 5)"
     ]
    }
   ],
   "source": [
    "###>>> start of solution\n",
    "\n",
    "# TODO add loss function!!!\n",
    "loss_fn = MSELoss()\n",
    "loss_fn.forward( predicted_Y_hat,dataset_labels_Y)\n",
    "mlp.backward(loss_fn.backward( predicted_Y_hat,dataset_labels_Y))\n",
    "###<<< end of solution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify your solution!\n",
    "gradient_check(mlp, loss_fn, dataset_features_X, dataset_labels_Y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
