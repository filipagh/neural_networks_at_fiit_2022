{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3 - Optimizers\n",
    "\n",
    "**Requirements:**\n",
    " - numpy (https://numpy.org/)\n",
    " - matplotlib (https://matplotlib.org/)\n",
    "or\n",
    " - Plotly (https://plotly.com/)\n",
    "\n",
    "Let's continue with our framework. We use all of the previous implemented classes (with some modifications) and add new - **Optimizers**.\n",
    "\n",
    "Watch out for the shape of input data.. Now we are working with mini-batches $(B,nX,1)$, where $B$ is number of samples in mini-batch, $nX$ is number of features and $1$ is for vector/matrix multiplication in the last 2 dimensions, leaving $B$ as samples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from utils import Module\n",
    "\n",
    "a = [1,2]\n",
    "a[1]= [\"a\",\"b\"]\n",
    "[x,y] = a[1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear layer (Dense, Fully connected, Single Layer Perceptron)\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.aPred = input\n",
    "        self.m = self.aPred.shape[0]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        self.dW = (1.0/self.m) * np.sum(np.matmul(dz, self.aPred.transpose((0,2,1))), axis=0)\n",
    "        self.db = (1.0/self.m) * np.sum(dz, axis=0)\n",
    "        return np.matmul(self.W.transpose(), dz)\n",
    "\n",
    "    def get_optimizer_context(self):\n",
    "        return [[self.W, self.dW], [self.b, self.db]]\n",
    "\n",
    "    def update_parameters(self, params):\n",
    "        self.W, self.b = params\n",
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, np.multiply(a, 1 - a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2 * input) - 1) / (np.exp(2 * input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, 1 - np.square(a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            z = module.backward(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Functions\n",
    "\n",
    "As in standard deep learning frameworks, calling Loss function can return either **cost** or  **loss**  based on parameter **reduce**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(MSELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(np.mean(np.power(target - input, 2), axis=0, keepdims=True))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(-2 * (target - input), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(BCELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(-(target * np.log(input) + np.multiply((1 - target), np.log(1 - input))))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -np.divide(target, input) + np.divide(1 - target, 1 - input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers\n",
    "\n",
    "Each optimizer gets as input a **model** and loads each layer's parameters for optimizer context **`layer.get_optimizer_context()`**. Other attributes are based on the optimizer definition. The modified parameters are put back to the model's layer by `layer.set_optimizer_context([W,b])`. Remember that optimizers may require to store some context for the next steps of optimization for each layer and each parameter accordingly.\n",
    "\n",
    "Your task is to implement:\n",
    " - SGD with momentum\n",
    " - RMSProp: http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf\n",
    " - Adam: https://arxiv.org/pdf/1412.6980.pdf\n",
    "\n",
    "All algorithms are in [https://www.deeplearningbook.org/contents/optimization.html](https://www.deeplearningbook.org/contents/optimization.html)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   AbstractOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   StochasticGradientDescentOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model:Model, lr:float):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.context = {}\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    # >>>> start here\n",
    "                    W = W - self.lr * dW\n",
    "                    b = b - self.lr * db\n",
    "                    # <<<< end here\n",
    "                    layer.update_parameters([W,b])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SGDMomentumOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, model, lr:float, momentum=0.5):  #momentum - beta |\n",
    "        super(SGDMomentum, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        # >>>> start_solution\n",
    "\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    if name in self.context.keys():\n",
    "                    # >>>> start_solution\n",
    "                        [vW,vb] = self.context[name]\n",
    "                    else:\n",
    "                        [vW,vb] = [dW, db]\n",
    "\n",
    "                    vW_actual = (1-self.momentum) * dW + self.momentum*vW\n",
    "                    vb_actual = (1-self.momentum) * db + self.momentum*vb\n",
    "                    self.context[name] =  [vW_actual,vb_actual]\n",
    "\n",
    "                    W = W - self.lr * vW_actual\n",
    "                    b = b - self.lr * vb_actual\n",
    "                    # <<<< end_solution\n",
    "                    layer.update_parameters([W,b])\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RMSpropOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, model,lr:float, momentum=0.5):\n",
    "        super(RMSprop, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        self.momentum = momentum\n",
    "        self.lr=lr\n",
    "        # >>>> start_solution\n",
    "\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    if name in self.context.keys():\n",
    "                        # >>>> start_solution\n",
    "                        [sW,sb] = self.context[name]\n",
    "                        sW_actual = (1-self.momentum) * (dW**2) + self.momentum*sW\n",
    "                        sb_actual = (1-self.momentum) * (db**2) + self.momentum*sb\n",
    "                    else:\n",
    "                        # [sW,sb] = [dW**2, db**2]\n",
    "                        [sW,sb] = [(1-self.momentum)* dW**2, (1-self.momentum)*db**2]\n",
    "                        sW_actual = sW\n",
    "                        sb_actual = sb\n",
    "\n",
    "                    self.context[name] =  [sW_actual, sb_actual]\n",
    "\n",
    "                    W = W - self.lr * (dW / np.sqrt(sW_actual))\n",
    "                    b = b - self.lr * (dW / np.sqrt(sb_actual))\n",
    "\n",
    "                    layer.update_parameters([W, b])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   AdamOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr:float, momentum_1=0.9, momentum_2 = 0.99):\n",
    "        super(Adam, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        self.lr = lr\n",
    "        self.momentum_1 = momentum_1\n",
    "        self.momentum_2 = momentum_2\n",
    "        self.beta_cache_1 = momentum_1\n",
    "        self.beta_cache_2 = momentum_2\n",
    "        # >>>> start_solution\n",
    "\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        # >>>>>> Probably add something here ;)\n",
    "\n",
    "        # <<<<<< until here\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    if name in self.context.keys():\n",
    "                                \n",
    "                        \n",
    "                        [s,r] = self.context[name]\n",
    "                        s = self.momentum_1*s + (1-self.momentum_1)*X\n",
    "                        r = self.momentum_2*r + (1-self.momentum_2)*(X**2)\n",
    "                        # >>>> start_solution\n",
    "                    #    self.context[name][???] =\n",
    "                        pass\n",
    "                    else:\n",
    "                        s = (1-self.momentum_1)*dW\n",
    "                        r = (1-self.momentum_2)*(dW**2)\n",
    "                        pass\n",
    "                    ss = s/(1-self.beta_cache_1)\n",
    "                    self.beta_cache_1 *= self.momentum_1\n",
    "                    rr = r/(1-self.beta_cache_2)\n",
    "                    self.beta_cache_2 *= self.momentum_2\n",
    "\n",
    "                    self.context[name] = [s,r]\n",
    "                    # <<<< end_solution\n",
    "\n",
    "                    layer.update_parameters([W, b])\n",
    "    def adam_part(self,dx, s= None,r=None): \n",
    "        if s is not None:\n",
    "        \n",
    "            pass\n",
    "        else:\n",
    "            pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Processing Cell\n",
    "\n",
    "Watch out for the shape of mini-batch (B,Features,1)\n",
    "\n",
    " 1. Initialize dataset (`dataset_Flower`).\n",
    " 2. Declare a simple model.\n",
    " 3. Initialize optimizer.\n",
    " 4. Make mini-batches.\n",
    " 5. Perform forward pass through the network.\n",
    " 6. Compute loss.\n",
    " 7. Backward prop loss.\n",
    " 8. Track loss.\n",
    " 9. Backward pass MLP.\n",
    " 10. Use optimizer to modify model parameters.\n",
    " 11. Repeat for $N$ epochs\n",
    " 12. Visualize other plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from utils import gradient_check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from dataset import dataset_Flower, MakeBatches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "dataset = MakeBatches(dataset_Flower(m=512, noise=0.3), 32, True)\n",
    "###>>> start of solution\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlp.add_module(Tanh(), 'Tanh_1')\n",
    "mlp.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlp.add_module(Tanh(), 'Tanh_2')\n",
    "mlp.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlp.add_module(Tanh(), 'Tanh_3')\n",
    "mlp.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlp.add_module(Sigmoid(), 'Sigmoid')\n",
    "loss_fn = MSELoss(reduce='mean')\n",
    "\n",
    "# optimizer = SGD(mlp, lr=0.001)\n",
    "# optimizer = SGDMomentum(mlp, lr=0.001,momentum=0.9)\n",
    "optimizer = RMSprop(mlp, lr=0.001,momentum=0.95)\n",
    "# optimizer = SGD(mlp, lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,4,2) (4,3) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mini_batch_X, mini_batch_Y \u001B[38;5;129;01min\u001B[39;00m dataset:\n\u001B[0;32m----> 6\u001B[0m     predicted_Y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mmlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch_X\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(predicted_Y_hat, mini_batch_Y)\n\u001B[1;32m      8\u001B[0m     epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mmean(loss)]\n",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36mModel.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m---> 65\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/IdeaProjects/neural_networks_at_fiit_2022/week_4/utils.py:78\u001B[0m, in \u001B[0;36mModule.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maPred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maPred\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m---> 13\u001B[0m net \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mb\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m net\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (32,4,2) (4,3) "
     ]
    }
   ],
   "source": [
    "N_epochs = 1000\n",
    "losses = []\n",
    "for i in range(N_epochs):\n",
    "    epoch_loss = []\n",
    "    for mini_batch_X, mini_batch_Y in dataset:\n",
    "        predicted_Y_hat = mlp.forward(mini_batch_X)\n",
    "        loss = loss_fn(predicted_Y_hat, mini_batch_Y)\n",
    "        epoch_loss += [np.mean(loss)]\n",
    "        dLoss = loss_fn.backward(predicted_Y_hat, mini_batch_Y)\n",
    "        mlp.backward(dLoss)\n",
    "        # gradient_check(mlp, loss_fn, mini_batch_X, mini_batch_Y)\n",
    "        optimizer.step()\n",
    "    losses += [np.mean(epoch_loss)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.line({'SGD':losses})\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}